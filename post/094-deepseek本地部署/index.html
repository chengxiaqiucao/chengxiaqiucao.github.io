<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="目前大模型应用如火如荼，各大LLM如Deepseek也都提供了在线的助手服务，结合mcp-server还可以进一步拓展到本地的工具能力。\n但对于一些和本地业务和数据强相关的资料，在线的大模型训练数据集一般并不能涵盖，特别还有一些敏感或对安全要求很高的数据，使用在线大模型并不现实。所以我们个人应用和实际工作中，本地部署大模型并加入本地知识库也是一个刚性需求。\n"><title>打造个人知识库，wsl+ollama部署deepseek与vscode集成</title><link rel=canonical href=https://chengxiaqiucao.github.io/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/><link rel=stylesheet href=/scss/style.min.d6334826c849a420a2748e4872b915ae516105f86c3993d9bad2032725f784b8.css><meta property='og:title' content="打造个人知识库，wsl+ollama部署deepseek与vscode集成"><meta property='og:description' content="目前大模型应用如火如荼，各大LLM如Deepseek也都提供了在线的助手服务，结合mcp-server还可以进一步拓展到本地的工具能力。\n但对于一些和本地业务和数据强相关的资料，在线的大模型训练数据集一般并不能涵盖，特别还有一些敏感或对安全要求很高的数据，使用在线大模型并不现实。所以我们个人应用和实际工作中，本地部署大模型并加入本地知识库也是一个刚性需求。\n"><meta property='og:url' content='https://chengxiaqiucao.github.io/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/'><meta property='og:site_name' content='秋 草 观 “测” 台'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='行业趋势'><meta property='article:tag' content='AI'><meta property='article:tag' content='测试环境'><meta property='article:published_time' content='2025-05-15T00:00:00+00:00'><meta property='article:modified_time' content='2025-05-15T00:00:00+00:00'><meta property='og:image' content='https://chengxiaqiucao.github.io/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/snap_20250419-3.png'><meta name=twitter:title content="打造个人知识库，wsl+ollama部署deepseek与vscode集成"><meta name=twitter:description content="目前大模型应用如火如荼，各大LLM如Deepseek也都提供了在线的助手服务，结合mcp-server还可以进一步拓展到本地的工具能力。\n但对于一些和本地业务和数据强相关的资料，在线的大模型训练数据集一般并不能涵盖，特别还有一些敏感或对安全要求很高的数据，使用在线大模型并不现实。所以我们个人应用和实际工作中，本地部署大模型并加入本地知识库也是一个刚性需求。\n"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://chengxiaqiucao.github.io/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/snap_20250419-3.png'></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/automnGrass_hu_5e44a092fd905642.png width=300 height=300 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>秋 草 观 “测” 台</a></h1><h2 class=site-description>Testing is not just checking...</h2></div></header><ol class=menu-social><li><a href=https://github.com/chengxiaqiucao target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://space.bilibili.com/472272606 target=_blank title=Bilibili rel=me><!doctype html><svg t="1750732982440" class="icon" viewBox="0 0 1024 1024" p-id="4593" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M306.005333 117.632 444.330667 256h135.296l138.368-138.325333A42.666667 42.666667.0 01778.368 178.048L700.330667 256H789.333333A149.333333 149.333333.0 01938.666667 405.333333v341.333334A149.333333 149.333333.0 01789.333333 896H234.666667A149.333333 149.333333.0 0185.333333 746.666667V405.333333A149.333333 149.333333.0 01234.666667 256h88.96L245.632 177.962667a42.666667 42.666667.0 0160.373333-60.373334zm483.328 223.701333H234.666667a64 64 0 00-63.701334 57.856L170.666667 405.333333v341.333334a64 64 0 0057.856 63.701333L234.666667 810.666667h554.666666a64 64 0 0063.701334-57.856L853.333333 746.666667V405.333333a64 64 0 00-64-64zm-448 128A42.666667 42.666667.0 01384 512v85.333333a42.666667 42.666667.0 01-85.333333.0V512a42.666667 42.666667.0 0142.666666-42.666667zm341.333334.0A42.666667 42.666667.0 01725.333333 512v85.333333a42.666667 42.666667.0 01-85.333333.0V512a42.666667 42.666667.0 0142.666667-42.666667z" p-id="4594" fill="#8a8a8a"/></svg></a></li><li><a href=https://www.zhihu.com/people/qiucao target=_blank title=zhihu rel=me><!doctype html><svg t="1750733049138" class="icon" viewBox="0 0 1024 1024" p-id="5820" xmlns:xlink="http://www.w3.org/1999/xlink" width="200" height="200"><path d="M570.581333 806.272h61.952L652.928 876.117333 764.074667 806.272h130.986666V230.186667h-324.48V806.272zM636.501333 292.693333h192.64V743.68h-73.898666l-73.813334 46.378667L668.032 743.808l-31.530667-.128V292.736zM515.754667 493.738667H377.429333a2999.466667 2999.466667.0 005.802667-194.56h135.338667S523.776 239.445334 495.872 240.128H261.76c9.216-34.730667 20.821333-70.613333 34.688-107.690667.0.0-63.701333.0-85.333333 57.130667C202.112 213.12 176.128 303.786667 129.877333 396.416c15.573333-1.706667 67.114667-3.114667 97.450667-58.794667 5.589333-15.616 6.656-17.621333 13.568-38.485333h76.373333c0 27.776-3.157333 177.109333-4.437333 194.474667h-138.24c-31.104.0-41.173333 62.549333-41.173333 62.549333h173.482666C295.253333 688.256 232.789333 799.573333 119.466667 887.466667c54.186667 15.488 108.202667-2.432 134.912-26.197334.0.0 60.8-55.338667 94.122666-183.381333L491.264 849.834667s20.906667-71.168-3.285333-105.856c-20.053333-23.637333-74.24-87.552-97.322667-110.72l-38.698667 30.72c11.52-36.992 18.474667-72.96 20.821334-107.690667h163.072s-.213333-62.549333-20.053334-62.549333z" p-id="5821" fill="#8a8a8a"/></svg></a></li><li><a href=mailto:%20danmyw@qq.com target=_blank title=e-mail rel=me><!doctype html><svg t="1750733089669" class="icon" viewBox="0 0 1451 1024" p-id="6983" xmlns:xlink="http://www.w3.org/1999/xlink" width="283.3984375" height="200"><path d="M664.781909 681.472759.0 97.881301C0 3.997201 71.046997.0 71.046997.0H474.477909 961.649408h399.992405s71.046998 3.997201 71.046998 97.881301L771.345323 681.472759S764.482731 685.154773 753.594283 688.65053V688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858V688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759zm53.281707 130.131124C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633.0 212.052267.0 212.052267V942.809523S0 1024 83.726336 1024H682.532949 753.579947h595.368192C1432.688811 1024 1432.688811 942.809523 1432.688811 942.809523V212.052267S893.138176 701.759633 817.019477 767.734955c-39.771477 34.470494-74.671786 43.295855-98.955861 43.868928z" fill="#8a8a8a" p-id="6984"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>主页</span></a></li><li><a href=/page/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>检索</span></a></li><li><a href=/page/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>链接</span></a></li><li><a href=/page/about/><svg class="icon icon-tabler icon-tabler-user" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="7" r="4"/><path d="M6 21v-2a4 4 0 014-4h4a4 4 0 014 4v2"/></svg>
<span>关于</span></a></li><li><a href=/page/releases/><svg class="icon icon-tabler icon-tabler-infinity" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M9.828 9.172a4 4 0 100 5.656A10 10 0 0012 12a10 10 0 012.172-2.828 4 4 0 110 5.656A10 10 0 0112 12 10 10 0 009.828 9.172"/></svg>
<span>热门开源</span></a></li><li class=menu-bottom-section><ol class=menu><li id=i18n-switch><svg class="icon icon-tabler icon-tabler-language" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M4 5h7"/><path d="M9 3v2c0 4.418-2.239 8-5 8"/><path d="M5 9c-.003 2.144 2.952 3.908 6.7 4"/><path d="M12 20l4-9 4 9"/><path d="M19.1 18h-6.2"/></svg>
<select name=language title=language onchange="window.location.href=this.selectedOptions[0].value"><option value=https://chengxiaqiucao.github.io/en/>English</option><option value=https://chengxiaqiucao.github.io/ selected>中文</option></select></li><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#ollama简介>Ollama简介</a></li><li><a href=#deepseek本地模型及运行配置>Deepseek本地模型及运行配置</a></li><li><a href=#利用ollama在wsl中部署本地deepseek>利用Ollama在WSL中部署本地Deepseek</a><ol><li><a href=#wsl配置>WSL配置</a></li><li><a href=#安装ollama>安装Ollama</a></li><li><a href=#部署deepseek>部署deepseek</a></li><li><a href=#添加知识库分析模型>添加知识库分析模型</a></li></ol></li><li><a href=#用cherry-studio管理知识库并提供本地交互界面>用Cherry Studio管理知识库并提供本地交互界面</a></li><li><a href=#集成本地deepseek到vscode>集成本地Deepseek到vscode</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/><img src=/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/snap_20250419-3_hu_22049cf2d6d4d691.png srcset="/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/snap_20250419-3_hu_22049cf2d6d4d691.png 800w, /post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/snap_20250419-3_hu_b86bb4c4d2cf9b6e.png 1600w" width=800 height=369 loading=lazy alt="Featured image of post 打造个人知识库，wsl+ollama部署deepseek与vscode集成"></a></div><div class=article-details><header class=article-category><a href=/categories/ai/ style=background-color:#245678;color:#888>AI</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/>打造个人知识库，wsl+ollama部署deepseek与vscode集成</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>May 15, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 4 分钟</time></div></footer></div></header><section class=article-content><p>目前大模型应用如火如荼，各大LLM如<code>Deepseek</code>也都提供了在线的助手服务，结合<code>mcp-server</code>还可以进一步拓展到本地的工具能力。</p><p>但对于一些和本地业务和数据强相关的资料，在线的大模型训练数据集一般并不能涵盖，特别还有一些敏感或对安全要求很高的数据，使用在线大模型并不现实。所以我们个人应用和实际工作中，本地部署大模型并加入本地知识库也是一个刚性需求。</p><p>本文就记录下在WSL中通过Ollama和CherryStudio搭建本地大模型，并将本地模型集成到VsCode的AI助手的过程分享。</p><h2 id=ollama简介>Ollama简介</h2><p>Ollama 是一个基于Go 语言开发的简单易用的本地大语言模型运行框架，专为在本地机器上便捷部署和运行大型语言模型（LLM）而设计。Ollama 是 <strong>Omni-Layer Learning Language Acquisition Model（全方位学习语言接受模型）</strong> 的简写。</p><h2 id=deepseek本地模型及运行配置>Deepseek本地模型及运行配置</h2><p>Deepseek目前根据本地部署包含的参数集大小，又包含 <strong>1.5B</strong> 到 <strong>671B</strong> 等多个版本，参数集越大则AI越智能，但相应地对硬件要求则越高。</p><p>一般对应不同deepseek模型版本，推荐的硬件配置如下：</p><div class=table-wrapper><table><thead><tr><th>模型型号</th><th>CPU</th><th>内存</th><th>硬盘</th><th>显存</th><th>适用场景</th></tr></thead><tbody><tr><td>DeepSeek-R1-1.5B</td><td>4 核</td><td>8 GB+</td><td>3 GB+</td><td>非必需（若需 CPU 加速可选 GTX 1650）</td><td>个人使用，如笔记本电脑、台式电脑等</td></tr><tr><td>DeepSeek-R1-7B</td><td>8 核</td><td>16 GB+</td><td>8 GB+</td><td>8 GB+显存（如 RTX 3070/4060）</td><td>中小型企业本地开发</td></tr><tr><td>DeepSeek-R1-8B</td><td>8 核</td><td>16 GB+</td><td>8 GB+</td><td>8 GB+显存</td><td>中小型企业本地开发（提升精度的轻量级任务）</td></tr><tr><td>DeepSeek-R1-14B</td><td>12 核</td><td>32 GB+</td><td>15 GB+</td><td>16 GB+显存（如RTX 4090 或 A5000）</td><td>中小型企业本地开发（中量级任务）</td></tr><tr><td>DeepSeek-R1-32B</td><td>16 核</td><td>64 GB+</td><td>30 GB+</td><td>24 GB+显存（如 A100 40 GB 或 RTX 3090）</td><td>专业领域任务，如医疗、科研、法律</td></tr><tr><td>DeepSeek-R1-70B</td><td>32 核</td><td>128 GB+</td><td>70 GB+</td><td>多卡并行（如 2x A100 80GB 或 4x RTX 4090）</td><td>大型企业或科研机构，专业领域任务处理</td></tr><tr><td>DeepSeek-R1-671B（满血版）</td><td>64 核</td><td>512 GB+</td><td>300 GB+</td><td>多卡并行（如 8x A100/H100）</td><td>国家级科研任务处理</td></tr><tr><td>对我们个人用户来说，根据自己的电脑配置情况，通常选择1.5B到14B。 当然如果不是高频使用本地模型，个人建议可以将配置和推荐模型降一档匹配。</td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table></div><h2 id=利用ollama在wsl中部署本地deepseek>利用Ollama在WSL中部署本地Deepseek</h2><p>因为笔者当前主要是希望将一些本地数据接入Deepseek，也没有高频使用本地LLM的需求。所有这里我选择部署 <strong>DeepSeek-R1-14B</strong>， 另外为了避免每次启动电脑都因为LLM运行占用大量资源，所以这里我希望在Windows电脑的WSL下运行LLM</p><h3 id=wsl配置>WSL配置</h3><p>如果当前系统还没有启用WSL，可以先启用（过程不再详述， Win11中运行WSL install [对应Linux发行版] 即可安装）</p><p>通过命令行查看<strong>WSL</strong>状态:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-batch data-lang=batch><span class=line><span class=cl>C<span class=p>:</span><span class=nl>\qiucao</span><span class=c1>&gt;wsl --status</span>
</span></span><span class=line><span class=cl>默认分发: Ubuntu-24.04
</span></span><span class=line><span class=cl>默认版本: 2
</span></span></code></pre></td></tr></table></div></div><p>WSL默认的网络模式是NAT，这里可以通过<code>WSL Setting</code>程序，修改下当前配置，改为<code>Mirrored</code>镜像模式，共用宿主机网络。</p><p><img src=/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/snap_20250419-1.png width=1607 height=969 srcset="/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/snap_20250419-1_hu_251e83a643f91563.png 480w, /post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/snap_20250419-1_hu_d3b3405b54f8e912.png 1024w" loading=lazy class=gallery-image data-flex-grow=165 data-flex-basis=398px></p><h3 id=安装ollama>安装Ollama</h3><p>进入Ollama官网(<a class=link href=https://ollama.com/ target=_blank rel=noopener>https://ollama.com/</a>)下载界面, 因为我们是在WSL中部署，所以选择Linux安装。</p><p><img src=/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/snap_20250419.png width=1885 height=936 srcset="/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/snap_20250419_hu_99722e55ae18c4a1.png 480w, /post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/snap_20250419_hu_89a14deb9f0b4320.png 1024w" loading=lazy class=gallery-image data-flex-grow=201 data-flex-basis=483px></p><p>执行如下命令，等待下载并自动安装完成。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>curl -fsSL https://ollama.com/install.sh <span class=p>|</span> sh
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 安装完成查看版本</span>
</span></span><span class=line><span class=cl>~$ ollama --version
</span></span><span class=line><span class=cl>ollama version is 0.6.5
</span></span></code></pre></td></tr></table></div></div><h3 id=部署deepseek>部署deepseek</h3><p>完成ollama安装后，就可以根据Ollama提供的大模型清单，选择对应大模型进行安装。目前支持的大模型清单，可以从官网如下地址查询：https://ollama.com/library</p><p>执行如下命令下载并运行deepseek</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>ollama run deepseek-r1:14b
</span></span></code></pre></td></tr></table></div></div><p><img src=/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/ollama_deepseek.png width=1193 height=243 srcset="/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/ollama_deepseek_hu_1026ff78b8603fed.png 480w, /post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/ollama_deepseek_hu_9d7ff43704722fde.png 1024w" loading=lazy class=gallery-image data-flex-grow=490 data-flex-basis=1178px></p><p>安装成功后可以提问测试一下deepseek是否已正常提供服务</p><p><img src=/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/snap_20250419-2.png width=1404 height=158 srcset="/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/snap_20250419-2_hu_ed2f4389c23dba39.png 480w, /post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/snap_20250419-2_hu_82a3907315705e92.png 1024w" loading=lazy class=gallery-image data-flex-grow=888 data-flex-basis=2132px></p><h3 id=添加知识库分析模型>添加知识库分析模型</h3><p>至此，Deepseek其实已在本地部署。但如果要通过Deepseek使用本地数据和文档，我们还需要部署一个<strong>语义向量模型</strong>（Embedding Model），用来将本地数据转化为大模型可分析的知识库数据。这里我们选择 <strong>bge-m3</strong> 模型，这是一个通用向量模型，可以支持多语言、长文本和多种检索方式。</p><p>执行：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>ollama pull bge-m3
</span></span></code></pre></td></tr></table></div></div><p><img src=/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/ollama_bge.png width=1141 height=198 srcset="/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/ollama_bge_hu_63310f4bc6d28a9e.png 480w, /post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/ollama_bge_hu_9a9373e160733582.png 1024w" loading=lazy class=gallery-image data-flex-grow=576 data-flex-basis=1383px></p><h2 id=用cherry-studio管理知识库并提供本地交互界面>用Cherry Studio管理知识库并提供本地交互界面</h2><p>安装完Deepseek后，虽然我们可以在命令界面下和deepseek进行交互，但还是不够友好，包括对本地知识库的管理也不够方便。所以这里我们还可以安装一个开源的本地AI助手和知识库客户端Cherry Studio。从官网（https://cherry-ai.com）下载windows版本安装即可。
<img src=/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/snap_20250419-3.png width=1869 height=861 srcset="/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/snap_20250419-3_hu_162bef18952f99b6.png 480w, /post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/snap_20250419-3_hu_de42598554b32e8a.png 1024w" loading=lazy class=gallery-image data-flex-grow=217 data-flex-basis=520px></p><p>打开Cherry Studio，按如下步骤添加我们刚刚通过ollama部署的Deepseek</p><p><img src=/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/cherry.png width=1337 height=821 srcset="/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/cherry_hu_650d185a5428fb5.png 480w, /post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/cherry_hu_2420da097208e561.png 1024w" loading=lazy class=gallery-image data-flex-grow=162 data-flex-basis=390px></p><p>会看到已经安装好的模型，添加进来
<img src=/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/cherry_model.png width=972 height=659 srcset="/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/cherry_model_hu_1427909c6f167c6d.png 480w, /post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/cherry_model_hu_50b783a5da211168.png 1024w" loading=lazy class=gallery-image data-flex-grow=147 data-flex-basis=353px>
然后就可以利用Cherry Studio的知识库管理，将我们需要加入的本地文档纳入大模型的数据集中。这里包括直接添加文件、直接指定目录、从网站采集等多种方式，非常方便。</p><p><img src=/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/add_knowledge.png width=1347 height=828 srcset="/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/add_knowledge_hu_a4e9beef670ffd2f.png 480w, /post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/add_knowledge_hu_1187620292c81a03.png 1024w" loading=lazy class=gallery-image data-flex-grow=162 data-flex-basis=390px></p><h2 id=集成本地deepseek到vscode>集成本地Deepseek到vscode</h2><p>而有了本地的LLM后，如果我们希望在vscode中使用，可以利用vscode 的 <code>AI Toolkit</code> 插件，按如下方式添加本地LLM</p><p><img src=/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/vs_code_1.png width=1914 height=959 srcset="/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/vs_code_1_hu_833830a04db48530.png 480w, /post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/vs_code_1_hu_f4c99c1236cd540c.png 1024w" loading=lazy class=gallery-image data-flex-grow=199 data-flex-basis=478px></p><p>vscode中测试下本地库的能力</p><p><img src=/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/vs_code_2.png width=1433 height=1009 srcset="/post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/vs_code_2_hu_7715fc702df564d2.png 480w, /post/094-deepseek%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2/vs_code_2_hu_c5b07401fbe6ecb3.png 1024w" loading=lazy class=gallery-image data-flex-grow=142 data-flex-basis=340px></p><p>以上就是我们部署本地Deepseek的实践分享</p></section><footer class=article-footer><section class=article-tags><a href=/tags/%E8%A1%8C%E4%B8%9A%E8%B6%8B%E5%8A%BF/>行业趋势</a>
<a href=/tags/ai/>AI</a>
<a href=/tags/%E6%B5%8B%E8%AF%95%E7%8E%AF%E5%A2%83/>测试环境</a></section></footer></article><aside class=related-content--wrapper><h2 class=section-title>相关文章</h2><div class=related-content><div class="flex article-list--tile"><article class=has-image><a href=/post/106-mcp_must_fail/><div class=article-image><img src=/post/106-mcp_must_fail/Pasted-20250629.cbb2969de7429824c4784403798c5164_hu_18bdf008e934f54a.png width=250 height=150 loading=lazy alt="Featured image of post MCP必将失败？关于MCP的质疑声" data-hash="md5-y7KWnedCmCTEeEQDeYxRZA=="></div><div class=article-details><h2 class=article-title>MCP必将失败？关于MCP的质疑声</h2></div></a></article><article class=has-image><a href=/post/103-gemini-cli/><div class=article-image><img src=/post/103-gemini-cli/Pasted-20250626.0b88b79f381a0b6da0e44bb359d1f4c6_hu_8e884ae2c0706630.png width=250 height=150 loading=lazy alt="Featured image of post Cursor天塌了，Google开源Gemini-cli，编程助手卷王来了" data-hash="md5-C4i3nzgaC22g5EuzWdH0xg=="></div><div class=article-details><h2 class=article-title>Cursor天塌了，Google开源Gemini-cli，编程助手卷王来了</h2></div></a></article><article class=has-image><a href=/post/093-cursor%E7%9A%8412rules/><div class=article-image><img src=/post/093-cursor%E7%9A%8412rules/cursor.c2d80e1e9fe3ba4853f358a91a07b7dc_hu_9745a1f984ae1e58.png width=250 height=150 loading=lazy alt="Featured image of post 测试开发如何善用Cursor？" data-hash="md5-wtgOHp/jukhT81ipGge33A=="></div><div class=article-details><h2 class=article-title>测试开发如何善用Cursor？</h2></div></a></article><article><a href=/post/080-%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B/><div class=article-details><h2 class=article-title>提示词工程权威指南：赋能软件测试的艺术与科学</h2></div></a></article><article class=has-image><a href=/post/115-zread_ai/><div class=article-image><img src=/post/115-zread_ai/title.74442ea6ca67a0e29249b91d8665d84d_hu_ab07491885917994.png width=250 height=150 loading=lazy alt="Featured image of post  快速理清开源项目源码的神器，这款AI工具不可不知！" data-hash="md5-dEQupspnoOKSSbkdhmXYTQ=="></div><div class=article-details><h2 class=article-title>快速理清开源项目源码的神器，这款AI工具不可不知！</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2023 -
2025 |by 城下秋草（公众号： 秋草说测试）</section><section class=powerby>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>